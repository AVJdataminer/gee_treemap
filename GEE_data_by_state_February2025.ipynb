{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive"
      ],
      "metadata": {
        "id": "rp7rB0PZ2Z8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxKRIgGd2Jp-",
        "outputId": "f135cbcf-a0d2-4d4d-f399-7547562339b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['grids.csv',\n",
              " 'fires_with_grids.csv',\n",
              " 'grids_with_pop_den_ids.csv',\n",
              " 'weather.csv',\n",
              " 'weather_locations.csv',\n",
              " 'grids_within_weather_buffers.csv',\n",
              " 'grids_with_weather_loc_id.csv',\n",
              " 'fires_sample_2022.csv',\n",
              " 'weather_with_loc.csv',\n",
              " 'daily_weather.csv',\n",
              " 'raw_data',\n",
              " 'clean_data',\n",
              " 'Firewatch-functions-to save.ipynb',\n",
              " 'fires_sample_2023_8.csv',\n",
              " 'sample_with_weather_2023-06-01_2023-08-30.csv',\n",
              " 'weather_formatted.csv',\n",
              " 'CO',\n",
              " 'nlcd',\n",
              " '.ipynb_checkpoints',\n",
              " 'Untitled0.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "working_directory = '/content/drive/MyDrive/FireWatch'  # Update this path\n",
        "os.chdir(working_directory)\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a 1 acre grid"
      ],
      "metadata": {
        "id": "M8uwgqsI3uZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import shapely.geometry as geom\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def generate_sd_grid():\n",
        "    # Download the US states shapefile from the U.S. Census Bureau.\n",
        "    # (This zip file contains a shapefile of all states.)\n",
        "    url = \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip\"\n",
        "    states = gpd.read_file(url)\n",
        "\n",
        "    # Filter for South Dakota\n",
        "    sd = states[states['NAME'] == 'South Dakota']\n",
        "\n",
        "    # Reproject to an equal-area projection (EPSG:5070) in meters so that our area calculations are correct.\n",
        "    sd = sd.to_crs(epsg=5070)\n",
        "\n",
        "    # Define the side length of a 1-acre square in meters.\n",
        "    # 1 acre â‰ˆ 4046.8564 square meters, so side length = sqrt(4046.8564)\n",
        "    cell_size = math.sqrt(4046.8564)\n",
        "\n",
        "    # Get the bounding box of South Dakota in the projected coordinate system.\n",
        "    minx, miny, maxx, maxy = sd.total_bounds\n",
        "\n",
        "    # Create lists of coordinates for the grid cells.\n",
        "    xs = np.arange(minx, maxx, cell_size)\n",
        "    ys = np.arange(miny, maxy, cell_size)\n",
        "\n",
        "    # Generate grid cells (as shapely polygons) covering the bounding box.\n",
        "    grid_cells = [geom.box(x, y, x + cell_size, y + cell_size) for x in xs for y in ys]\n",
        "\n",
        "    # Create a GeoDataFrame from the grid cells.\n",
        "    grid = gpd.GeoDataFrame({'geometry': grid_cells}, crs=sd.crs)\n",
        "\n",
        "    # Clip the grid to only those cells that intersect South Dakota.\n",
        "    grid_clipped = gpd.overlay(grid, sd, how='intersection')\n",
        "\n",
        "    return grid_clipped\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    sd_grid = generate_sd_grid()\n",
        "    print(sd_grid.head())\n",
        "    # Optionally, save the grid to a file (e.g., as a shapefile or GeoPackage)\n",
        "    # sd_grid.to_file(\"south_dakota_1acre_grid.shp\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "H-aVa2VQ2fKo",
        "outputId": "a52e7d38-ca2e-4336-962c-573d6e729ee7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd129dd6036e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0msd_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sd_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd129dd6036e>\u001b[0m in \u001b[0;36mgenerate_sd_grid\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Generate grid cells (as shapely polygons) covering the bounding box.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgrid_cells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd129dd6036e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Generate grid cells (as shapely polygons) covering the bounding box.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgrid_cells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shapely/geometry/geo.py\u001b[0m in \u001b[0;36mbox\u001b[0;34m(minx, miny, maxx, maxy, ccw)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shapely/geometry/polygon.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(self, shell, holes)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mgeom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mholes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shapely/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shapely/creation.py\u001b[0m in \u001b[0;36mpolygons\u001b[0;34m(geometries, holes, indices, out, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeometries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3572\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3574\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subset the grid for efficiency"
      ],
      "metadata": {
        "id": "ZyRXdIDS4lDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_western_quarter(grid):\n",
        "    # Compute the total bounding box of the grid (in the grid's CRS)\n",
        "    minx, miny, maxx, maxy = grid.total_bounds\n",
        "\n",
        "    # Define the threshold x coordinate as 1/4 of the distance from minx to maxx\n",
        "    x_threshold = minx + (maxx - minx) / 4\n",
        "\n",
        "    # Select cells where the centroid's x coordinate is less than or equal to the threshold\n",
        "    western_cells = grid[grid.geometry.centroid.x <= x_threshold]\n",
        "\n",
        "    return western_cells\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    sd_grid = generate_sd_grid()  # assuming you have the grid generation function from earlier\n",
        "    western_quarter = select_western_quarter(sd_grid)\n",
        "    print(western_quarter.head())\n"
      ],
      "metadata": {
        "id": "WrZxST2232k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query GEE by State for MODIS + Landsat + Treemap\n",
        "\n",
        "*   MODIS NDVI, EVI, LST, DEM, Aspect & Slope\n",
        "*   Harmonized Landsat & Sentinel-2\n",
        "*   Treemap Biomass Characteristics\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VmEuoqtz5c47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "\n",
        "# Initialize Earth Engine (authenticate if needed)\n",
        "ee.Initialize()\n",
        "\n",
        "def get_state_geometry(state_name):\n",
        "    \"\"\"\n",
        "    Given a state name, returns its geometry from the TIGER/2018/States collection.\n",
        "    \"\"\"\n",
        "    states = ee.FeatureCollection(\"TIGER/2018/States\")\n",
        "    state_feature = states.filter(ee.Filter.eq('NAME', state_name)).first()\n",
        "    return state_feature.geometry()\n",
        "\n",
        "def get_western_half_geometry(state_geometry):\n",
        "    \"\"\"\n",
        "    Computes a rectangular region representing the western half of the state's bounding box.\n",
        "    \"\"\"\n",
        "    state_bounds = state_geometry.bounds()\n",
        "    coords = ee.List(state_bounds.coordinates().get(0))\n",
        "    # Assume the first coordinate is bottom-left and the third is top-right.\n",
        "    bl = ee.List(coords.get(0))  # [minx, miny]\n",
        "    tr = ee.List(coords.get(2))  # [maxx, maxy]\n",
        "    minx = ee.Number(bl.get(0))\n",
        "    miny = ee.Number(bl.get(1))\n",
        "    maxx = ee.Number(tr.get(0))\n",
        "    maxy = ee.Number(tr.get(1))\n",
        "    mid_x = minx.add(maxx).divide(2)\n",
        "    western_half = ee.Geometry.Rectangle([minx, miny, mid_x, maxy])\n",
        "    return western_half\n",
        "\n",
        "def export_mod13a1_ndvi_evi(state_name, region, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Exports a mean composite of MODIS MOD13A1 NDVI and EVI for the specified region and time period.\n",
        "    \"\"\"\n",
        "    mod13a1 = ee.ImageCollection(\"MODIS/006/MOD13A1\") \\\n",
        "                .filterDate(start_date, end_date) \\\n",
        "                .select(['NDVI', 'EVI'])\n",
        "    mod13a1_mean = mod13a1.mean().clip(region)\n",
        "\n",
        "    state_clean = state_name.replace(\" \", \"_\")\n",
        "    file_prefix = f\"mod13a1_{state_clean}\"\n",
        "\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=mod13a1_mean,\n",
        "        description=f\"{state_clean}_MOD13A1_mean_NDVI_EVI\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=file_prefix,\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=500,\n",
        "        crs='EPSG:4326',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task for MOD13A1 NDVI/EVI for {state_name}.\")\n",
        "    return task\n",
        "\n",
        "def export_mod11a1_lst(state_name, region, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Exports a mean composite of MODIS MOD11A1 LST for the specified region and time period.\n",
        "    \"\"\"\n",
        "    mod11a1 = ee.ImageCollection(\"MODIS/006/MOD11A1\") \\\n",
        "                .filterDate(start_date, end_date) \\\n",
        "                .select('LST_Day_1km')\n",
        "    mod11a1_mean = mod11a1.mean().clip(region)\n",
        "\n",
        "    state_clean = state_name.replace(\" \", \"_\")\n",
        "    file_prefix = f\"mod11a1_{state_clean}\"\n",
        "\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=mod11a1_mean,\n",
        "        description=f\"{state_clean}_MOD11A1_mean_LST\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=file_prefix,\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=1000,\n",
        "        crs='EPSG:4326',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task for MOD11A1 LST for {state_name}.\")\n",
        "    return task\n",
        "\n",
        "def export_dem_slope_aspect(state_name, region):\n",
        "    \"\"\"\n",
        "    Exports DEM, slope, and aspect derived from the SRTM DEM for the specified region.\n",
        "    \"\"\"\n",
        "    dem = ee.Image(\"USGS/SRTMGL1_003\").clip(region)\n",
        "    slope = ee.Terrain.slope(dem)\n",
        "    aspect = ee.Terrain.aspect(dem)\n",
        "    dem_aspect_slope = dem.select([0], ['DEM']) \\\n",
        "                          .addBands(slope.rename('slope')) \\\n",
        "                          .addBands(aspect.rename('aspect'))\n",
        "\n",
        "    state_clean = state_name.replace(\" \", \"_\")\n",
        "    file_prefix = f\"dem_slope_aspect_{state_clean}\"\n",
        "\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=dem_aspect_slope,\n",
        "        description=f\"{state_clean}_DEM_Slope_Aspect\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=file_prefix,\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=30,\n",
        "        crs='EPSG:4326',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task for DEM, slope, and aspect for {state_name}.\")\n",
        "    return task\n",
        "\n",
        "def export_hls(state_name, region, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Exports a median composite of the harmonized Landsat-Sentinel-2 (HLS) product for the specified region and time period.\n",
        "    \"\"\"\n",
        "    hls = ee.ImageCollection(\"projects/USDA/NASS/HLS\") \\\n",
        "            .filterDate(start_date, end_date) \\\n",
        "            .filterBounds(region) \\\n",
        "            .median() \\\n",
        "            .clip(region)\n",
        "\n",
        "    state_clean = state_name.replace(\" \", \"_\")\n",
        "    file_prefix = f\"hls_{state_clean}\"\n",
        "\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=hls,\n",
        "        description=f\"{state_clean}_HLS_median\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=file_prefix,\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=30,\n",
        "        crs='EPSG:4326',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task for HLS for {state_name}.\")\n",
        "    return task\n",
        "\n",
        "def export_treemap(state_name, region):\n",
        "    \"\"\"\n",
        "    Exports the Treemap layers for the specified region using the\n",
        "    ee.ImageCollection('USFS/GTAC/TreeMap/v2016') asset. The layers include:\n",
        "      - BALIVE (Live Tree Basal Area)\n",
        "      - CANOPYPCT (Live Canopy Cover)\n",
        "      - CARBON_D (Carbon, Standing Dead)\n",
        "      - CARBON_DWN (Carbon, Down Dead)\n",
        "      - DRYBIO_D (Dry Standing Dead Tree Biomass)\n",
        "      - DRYBIO_L (Dry Live Tree Biomass)\n",
        "      - FLDSZCD (Field Stand-Size Class Code)\n",
        "    \"\"\"\n",
        "    treemap_ic = ee.ImageCollection('USFS/GTAC/TreeMap/v2016').filterBounds(region)\n",
        "    treemap = treemap_ic.mosaic() \\\n",
        "                .select([\"BALIVE\", \"CANOPYPCT\", \"CARBON_D\", \"CARBON_DWN\", \"DRYBIO_D\", \"DRYBIO_L\", \"FLDSZCD\"]) \\\n",
        "                .clip(region)\n",
        "\n",
        "    state_clean = state_name.replace(\" \", \"_\")\n",
        "    file_prefix = f\"treemap_{state_clean}\"\n",
        "\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=treemap,\n",
        "        description=f\"{state_clean}_TREEMAP_layers\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=file_prefix,\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=30,\n",
        "        crs='EPSG:4326',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task for Treemap layers for {state_name}.\")\n",
        "    return task\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # User-specified parameters\n",
        "    state_name = \"South Dakota\"  # Change this to any valid state name from TIGER/2018/States\n",
        "    start_date = '2020-01-01'\n",
        "    end_date = '2020-12-31'\n",
        "\n",
        "    # Retrieve the state geometry and compute the western half of the state's bounding box.\n",
        "    state_geom = get_state_geometry(state_name)\n",
        "    region = get_western_half_geometry(state_geom)\n",
        "\n",
        "    # Launch export tasks for each dataset.\n",
        "    task1 = export_mod13a1_ndvi_evi(state_name, region, start_date, end_date)\n",
        "    task2 = export_mod11a1_lst(state_name, region, start_date, end_date)\n",
        "    task3 = export_dem_slope_aspect(state_name, region)\n",
        "    task4 = export_hls(state_name, region, start_date, end_date)\n",
        "    task5 = export_treemap(state_name, region)\n",
        "\n",
        "    print(\"All export tasks have been started. Monitor the Earth Engine Tasks tab for progress.\")\n"
      ],
      "metadata": {
        "id": "ZCR0PsWW_BXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Indices from Landsat/Sentinel Data"
      ],
      "metadata": {
        "id": "PIFCH-JeBNKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_hls_indices(hls_image):\n",
        "    \"\"\"\n",
        "    Calculates a suite of vegetation and fire-related indices from a harmonized Landsat-Sentinel-2 (HLS) image.\n",
        "    Assumes the following bands are present in the image:\n",
        "      - 'B' for Blue\n",
        "      - 'G' for Green\n",
        "      - 'R' for Red\n",
        "      - 'N' for Near-Infrared (NIR)\n",
        "      - 'RE' for Red Edge\n",
        "      - 'SWIR1' for Shortwave Infrared\n",
        "\n",
        "    The calculated indices are:\n",
        "      1. NDVI: (N - R) / (N + R)\n",
        "      2. GNDVI: (N - G) / (N + G)\n",
        "      3. BNDVI: (N - B) / (N + B)\n",
        "      4. LCI: (N / G) - 1\n",
        "      5. MCARI: [ (RE - R) - 0.2*(R - G) ] * (RE / R)\n",
        "      6. NDRE: (N - RE) / (N + RE)\n",
        "      7. NDWI: (N - SWIR1) / (N + SWIR1)\n",
        "      8. SIPI: (N - B) / (N - R)\n",
        "      9. TGI: -0.5 * [660*(R - G) - 450*(R - B)]\n",
        "      10. VARI: (G - R) / (G + R - B)\n",
        "      11. NBR: (N - SWIR1) / (N + SWIR1)\n",
        "\n",
        "    Returns:\n",
        "      An ee.Image with each index as a separate band.\n",
        "    \"\"\"\n",
        "    # 1. NDVI = (N - R) / (N + R)\n",
        "    ndvi = hls_image.normalizedDifference(['N', 'R']).rename('NDVI')\n",
        "\n",
        "    # 2. GNDVI = (N - G) / (N + G)\n",
        "    gndvi = hls_image.normalizedDifference(['N', 'G']).rename('GNDVI')\n",
        "\n",
        "    # 3. BNDVI = (N - B) / (N + B)\n",
        "    bndvi = hls_image.normalizedDifference(['N', 'B']).rename('BNDVI')\n",
        "\n",
        "    # 4. LCI = (N / G) - 1\n",
        "    lci = hls_image.select('N').divide(hls_image.select('G')).subtract(1).rename('LCI')\n",
        "\n",
        "    # 5. MCARI = [ (RE - R) - 0.2*(R - G) ] * (RE / R)\n",
        "    mcari = hls_image.expression(\n",
        "        '((RE - R) - 0.2 * (R - G)) * (RE / R)',\n",
        "        {\n",
        "            'RE': hls_image.select('RE'),\n",
        "            'R': hls_image.select('R'),\n",
        "            'G': hls_image.select('G')\n",
        "        }\n",
        "    ).rename('MCARI')\n",
        "\n",
        "    # 6. NDRE = (N - RE) / (N + RE)\n",
        "    ndre = hls_image.normalizedDifference(['N', 'RE']).rename('NDRE')\n",
        "\n",
        "    # 7. NDWI = (N - SWIR1) / (N + SWIR1)\n",
        "    ndwi = hls_image.normalizedDifference(['N', 'SWIR1']).rename('NDWI')\n",
        "\n",
        "    # 8. SIPI = (N - B) / (N - R)\n",
        "    sipi = hls_image.expression(\n",
        "        '(N - B) / (N - R)',\n",
        "        {\n",
        "            'N': hls_image.select('N'),\n",
        "            'B': hls_image.select('B'),\n",
        "            'R': hls_image.select('R')\n",
        "        }\n",
        "    ).rename('SIPI')\n",
        "\n",
        "    # 9. TGI = -0.5 * [660*(R - G) - 450*(R - B)]\n",
        "    tgi = hls_image.expression(\n",
        "        '-0.5 * (660 * (R - G) - 450 * (R - B))',\n",
        "        {\n",
        "            'R': hls_image.select('R'),\n",
        "            'G': hls_image.select('G'),\n",
        "            'B': hls_image.select('B')\n",
        "        }\n",
        "    ).rename('TGI')\n",
        "\n",
        "    # 10. VARI = (G - R) / (G + R - B)\n",
        "    vari = hls_image.expression(\n",
        "        '(G - R) / (G + R - B)',\n",
        "        {\n",
        "            'G': hls_image.select('G'),\n",
        "            'R': hls_image.select('R'),\n",
        "            'B': hls_image.select('B')\n",
        "        }\n",
        "    ).rename('VARI')\n",
        "\n",
        "    # 11. NBR = (N - SWIR1) / (N + SWIR1)\n",
        "    nbr = hls_image.normalizedDifference(['N', 'SWIR1']).rename('NBR')\n",
        "\n",
        "    # Combine all indices into one multi-band image.\n",
        "    indices = ndvi.addBands(gndvi) \\\n",
        "                  .addBands(bndvi) \\\n",
        "                  .addBands(lci) \\\n",
        "                  .addBands(mcari) \\\n",
        "                  .addBands(ndre) \\\n",
        "                  .addBands(ndwi) \\\n",
        "                  .addBands(sipi) \\\n",
        "                  .addBands(tgi) \\\n",
        "                  .addBands(vari) \\\n",
        "                  .addBands(nbr)\n",
        "\n",
        "    return indices\n",
        "\n",
        "def export_hls_indices(hls_image, region, file_prefix, scale=30):\n",
        "    \"\"\"\n",
        "    Calculates indices from the provided HLS image and exports the results as a GeoTIFF to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "      hls_image: The harmonized Landsat-Sentinel-2 image (from Task 4 of your earlier query scripts).\n",
        "      region: An ee.Geometry defining the export region.\n",
        "      file_prefix: A string to be used as part of the file name (e.g., a state name).\n",
        "      scale: The export resolution in meters (default is 30).\n",
        "\n",
        "    Returns:\n",
        "      The export task.\n",
        "    \"\"\"\n",
        "    # Compute the indices image.\n",
        "    indices_image = calculate_hls_indices(hls_image)\n",
        "\n",
        "    # Set up and start the export task.\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=indices_image,\n",
        "        description=f\"{file_prefix}_HLS_Indices\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=f\"{file_prefix}_HLS_Indices\",\n",
        "        region=region.getInfo()['coordinates'],\n",
        "        scale=scale,\n",
        "        crs='EPSG:4326',\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Export task for HLS indices started with prefix: {file_prefix}_HLS_Indices\")\n",
        "    return task\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # In your workflow, the hls_image should come from Task 4 in your earlier query scripts.\n",
        "    # For this example, we create a dummy HLS composite by filtering the HLS collection.\n",
        "    start_date = '2020-01-01'\n",
        "    end_date = '2020-12-31'\n",
        "\n",
        "    # Define an example region; in your case, use your region of interest.\n",
        "    region = ee.Geometry.Rectangle([-102.0, 43.0, -101.0, 44.0])\n",
        "\n",
        "    # Retrieve the HLS image (from Task 4) by creating a median composite.\n",
        "    hls_collection = ee.ImageCollection(\"projects/USDA/NASS/HLS\") \\\n",
        "                        .filterDate(start_date, end_date) \\\n",
        "                        .filterBounds(region)\n",
        "    hls_image = hls_collection.median().clip(region)\n",
        "\n",
        "    # Define the file prefix (for example, the state name).\n",
        "    file_prefix = \"ExampleState\"  # Replace with your desired prefix.\n",
        "\n",
        "    # Calculate indices and export as a TIFF.\n",
        "    export_task = export_hls_indices(hls_image, region, file_prefix, scale=30)\n"
      ],
      "metadata": {
        "id": "1qx7T7TPCIOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GOES GLM Lightning"
      ],
      "metadata": {
        "id": "c1nOGXQHGOu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Snotel data"
      ],
      "metadata": {
        "id": "CaWfl2BhGU84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MERRA Airquality data"
      ],
      "metadata": {
        "id": "Umi1vSBgGzSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wildfire foot prints by state"
      ],
      "metadata": {
        "id": "NmLcdqaXHZX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "\n",
        "# Initialize Earth Engine (authenticate if needed)\n",
        "ee.Initialize()\n",
        "\n",
        "def get_state_geometry(state_name):\n",
        "    \"\"\"\n",
        "    Retrieves the geometry for a given state from the TIGER/2018/States collection.\n",
        "    \"\"\"\n",
        "    states = ee.FeatureCollection(\"TIGER/2018/States\")\n",
        "    state_feature = states.filter(ee.Filter.eq('NAME', state_name)).first()\n",
        "    return state_feature.geometry()\n",
        "\n",
        "def get_wildfire_footprints_by_state(state_name, start_date, end_date, scale=500):\n",
        "    \"\"\"\n",
        "    Retrieves wildfire footprints for a specified state and time period using\n",
        "    the MODIS Burned Area product (MCD64A1). The function creates a composite\n",
        "    of the burned area images, generates a binary burned mask (pixels where\n",
        "    BurnDate > 0), and converts the mask into polygon features.\n",
        "\n",
        "    Parameters:\n",
        "      state_name: A string (e.g., \"California\").\n",
        "      start_date: The start date (YYYY-MM-DD) for the query.\n",
        "      end_date: The end date (YYYY-MM-DD) for the query.\n",
        "      scale: The spatial resolution in meters (default is 500 m).\n",
        "\n",
        "    Returns:\n",
        "      An ee.FeatureCollection of wildfire footprint polygons.\n",
        "    \"\"\"\n",
        "    # Get the state geometry.\n",
        "    state_geom = get_state_geometry(state_name)\n",
        "\n",
        "    # Load the MODIS burned area collection and filter by date and state.\n",
        "    burned_collection = ee.ImageCollection(\"MODIS/006/MCD64A1\") \\\n",
        "                          .filterDate(start_date, end_date) \\\n",
        "                          .filterBounds(state_geom)\n",
        "\n",
        "    # Mosaic the collection into one image and clip to the state.\n",
        "    burned_image = burned_collection.mosaic().clip(state_geom)\n",
        "\n",
        "    # Create a binary mask where BurnDate > 0 (i.e., burned pixels).\n",
        "    burned_mask = burned_image.select(\"BurnDate\").gt(0)\n",
        "\n",
        "    # Convert the binary burned mask to vectors (polygons).\n",
        "    wildfire_footprints = burned_mask.reduceToVectors(\n",
        "        geometry=state_geom,\n",
        "        scale=scale,\n",
        "        geometryType='polygon',\n",
        "        eightConnected=True,\n",
        "        labelProperty='burned',\n",
        "        reducer=ee.Reducer.first()\n",
        "    )\n",
        "\n",
        "    return wildfire_footprints\n",
        "\n",
        "def export_wildfire_footprints(state_name, start_date, end_date, scale=500):\n",
        "    \"\"\"\n",
        "    Exports wildfire footprints for a given state and time period as a shapefile to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "      state_name: A string (e.g., \"California\").\n",
        "      start_date: The start date (YYYY-MM-DD) for the query.\n",
        "      end_date: The end date (YYYY-MM-DD) for the query.\n",
        "      scale: The spatial resolution in meters (default is 500 m).\n",
        "\n",
        "    Returns:\n",
        "      The export task.\n",
        "    \"\"\"\n",
        "    footprints = get_wildfire_footprints_by_state(state_name, start_date, end_date, scale)\n",
        "    state_clean = state_name.replace(\" \", \"_\")\n",
        "\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=footprints,\n",
        "        description=f\"{state_clean}_Wildfire_Footprints\",\n",
        "        folder='EarthEngineExports',\n",
        "        fileNamePrefix=f\"{state_clean}_Wildfire_Footprints\",\n",
        "        fileFormat='SHP'\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Export task for wildfire footprints of {state_name} started.\")\n",
        "    return task\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example parameters (adjust as needed)\n",
        "    state_name = \"California\"  # Change to your state of interest\n",
        "    start_date = \"2020-01-01\"\n",
        "    end_date = \"2020-12-31\"\n",
        "\n",
        "    # Retrieve wildfire footprints and print the number of features\n",
        "    footprints = get_wildfire_footprints_by_state(state_name, start_date, end_date)\n",
        "    print(\"Number of wildfire footprint features:\", footprints.size().getInfo())\n",
        "\n",
        "    # Export the wildfire footprints to Google Drive as a shapefile.\n",
        "    export_task = export_wildfire_footprints(state_name, start_date, end_date)\n"
      ],
      "metadata": {
        "id": "2YSUqSQPGNiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}